\documentclass[a4paper,12pt]{article}
\usepackage[swedish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{algorithmic}
\usepackage{amsmath, amsthm, amssymb}
% Ändra INTE nästa rad (säger var texten ska typsättas)
\usepackage[a4paper,includeheadfoot,margin=2.54cm]{geometry}
% Ändra INTE nästa rad (som lägger till radnummer till vänster)
\usepackage[left]{lineno}


% Ändra INTE raderna nedan
% Koden är från https://tex.stackexchange.com/questions/43648/
% Den fixar radnumrering av text i närvaro av matematikomgivningar
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
  \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
  \renewenvironment{#1}%
     {\linenomath\csname old#1\endcsname}%
     {\csname oldend#1\endcsname\endlinenomath}}% 
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
  \patchAmsMathEnvironmentForLineno{#1}%
  \patchAmsMathEnvironmentForLineno{#1*}}%
\AtBeginDocument{%
\patchBothAmsMathEnvironmentsForLineno{equation}%
\patchBothAmsMathEnvironmentsForLineno{align}%
\patchBothAmsMathEnvironmentsForLineno{flalign}%
\patchBothAmsMathEnvironmentsForLineno{alignat}%
\patchBothAmsMathEnvironmentsForLineno{gather}%
\patchBothAmsMathEnvironmentsForLineno{multline}%
}


% Ändra INTE nästa rad (gör så radnummer skrivs med fet stil)
\renewcommand\linenumberfont{\normalfont\bfseries\small}


\title{Algoritmer}
%
\author{Emil Wiklund\thanks{email: \texttt{emiwik-9@student.ltu.se}}\\  
        ~ \\
        Luleå tekniska universitet \\ 
        971 87 Luleå, Sverige}
%          
\date{\today}


\begin{document}


% Ändra INTE nästa rad (aktiverar utskrift av radnummer)
\linenumbers


\maketitle


\begin{abstract}
%
Artikeln är baserad på Jon Betley's artikel från september 1983 som beskriver 
den vardagliga verkan som design av algoritmer kan ha på programmerare. 
Från en algoritmisk synvinkel på problemet kan man se att dessa kan göra 
ett program enklare att förstå och skriva. I denna del har vi studerat ett 
bidrag inom ämnet sofistikerade algoritm metoder, dessa kan ibland leda till
en stor ökning prestandamässigt. 
%
\end{abstract}


\section{Introduktion}
\label{sec:introduktion}


Denna delen är framtagen kring ett mindre
problem med betoning på de algoritmer som ska lösa dessa problem samt
teknikerna på hur man designar dom. Några av algoritmerna är lite mer 
komplicerade men dessa motverkas med hjälp av “big-oh” metoden. Det
kommer vara tre stycken olika algoritmer som vi går igenom i denna artikel.
Anledningen till varför vi väljer att undersöka detta är pågrund utav att
intresset för snabbare program. Dessa program vill man alltså ska vara 
skrivna så att prestandan höjs.
%
%Här kan man skriva massor om uppgiften, dess bakgrund, varför det är
%viktiga att lösa det osv men för oss räcker det med att introducera
%den. Ett bra sätt att skriva på är att sen behandla varje problem i
%egna avsnitt, där man inleder med att beskriva problemet noga innan
%man visar hur man kan lösa det.
%Faktum är att det blir lite krystat att skriva en liten rapport om hur
%man löser en uppgift som denna men vi gör det som en övning i \LaTeX. 

\section{Big-Oh}
\label{sec:big-oh}


Betdyelsen bakom begreppet “big-oh” är att det står för en förenklad 
analys av en algoritms effektivitet.
Detta begrepp ger oss en algoritms komplexitet baserat på invärdet, 
detta kallar vi för N. Det ger oss ett sätt att kunna räkna ut hur effektiv
våra algortimer är. Man kan använda “big-oh” för att mäta körtiden och 
storleken på objekt. Med “big-oh” kan man alltså beräkna den sämsta 
körtiden, den bästa körtiden samt en körstid som har ett medelvärde. 
Generella regler som “big-oh” håller sig till är för det första att den 
ignorerar konstanter när N blir för stort för då så har konstanten 
nästan ingen betydelse. För det andra så är olika termer överlägsna andra, 
exempelvis så är $O(N^2)$ mindre dominant än $O(N^n)$ det finns
däremot flera termer och dessa är:
%
\begin{center}
$O(1) < O(log_N) < O(N) < O(Nlog_N) < O(N^2) < O(2^N) < O(N!).$
\end{center}
%
Ett exempel där man kör kod med “big-oh” kan se ut så här:
%
\begin{center}
\begin{algorithmic}
 \FOR{x in range (O,N)}
 	\FOR{y in range (O, N)}
  		  \STATE{print(x$\cdot$y)}
 	 \ENDFOR
 \ENDFOR
\end{algorithmic}
\end{center}
%
Det som sker är att snurrorna kommer att byta ut $x$ och $y$ mot värdena 
inuti snurran för $x$ respektive $y$ innanför parantersen, i detta fall med 
ett naturligt tal N. Därav kommer print satsen att bli till $O(N^2)$ eftersom
det blir O(N) multiplicerat med O(N) och detta är alltså körtiden för denna
kod.$O(N^2)$
%


\section{Problemet och ett simpelt program}
\label{sec:uppg1}


Problemet uppstod i en endimensionellt mönsterigenkänning. Historien beskrivs
senare. Invärdet är en talföljd $X$ av $N$ naturliga tal. Dess utvärde är den
maximala
summan inom en delad under talföljd av invärdet. Exempelvis om invärdet är $3$
och
$4$ ur listan med nummer:
%
\begin{align*}
  [31, -41, 59, 26, -53, 58, 97, -93, -23, 84]
\end{align*}
%
kommer programmet att returnera summan av $X[3...7]$, dvs $187$. Problemet är
enkelt
när alla nummer är positiva, den största understa talföljden är lika med
talföljden
med invärden.
Problemet uppstår när siffror är negativa. Skulle vi inkludera ett negativt
nummer skulle man kunna hoppas på att ett positivt nummer skulle kunna ta ut
detnegativa eller med andra ord kompensera. Om alla inputs skulle vara negativa
nummer skulle summan av den undre talföljden vara noll vilket också ger den
totala
summan $0$.
%
Det programmet man vill använda för detta är ett simpelt program som kör for
each på  de par av heltalen $L$ och $U$ där $1 \leq L \leq U \leq N$. Detta
beräknar summan
av $X[L...U]$ och gör en kontroll om summan är större än den summan som är
störst
inuläget. Koden som visas i Algorithm $1$ är kort och enkel att förstå.
Däremotså
är den långsam. Koden tar kring $1$ timme att köra om $N$ är $1000$ och $39$
dagar om $N$ är $10000$. Nedan ser vi koden för Algoritm 1:
%
\begin{center}
%
\begin{algorithmic}
\newpage
\STATE{MaxSoFar := 0.0}
\FOR{L := 1 to N}
 	\FOR{U := L to N}
   	\STATE{Sum := 0.0}
   		\FOR{I := L to U}
  		  \STATE{Sum := Sum + X[I]}
  	 \ENDFOR
  	 \STATE{/* Sum behåller nu summan av $X[L..U]$} */
  	 \STATE{MaxSoFar := max(MaxSoFar, Sum)}
 	 \ENDFOR
 \ENDFOR
\end{algorithmic}
%
\end{center}
%


\section{Två kvadratiska algoritmer}
\label{sec:uppgN}


\subsection{Algoritm 2}


Vi får en annan känsla för algoritmer och hur effektiva man skulle kunna göra
dom. Detta kan vi göra genom att använda oss utav “big-oh”,
beteckningen\footnote{$O(N^2)$ kan ses som proportionerligt till $N^2$; både
$15N^2 + 100N$ och $N^2/2 -10$ är $O(N^2)$. Sen så $f(N) = O(g(N))$ betyder att
$f(N) < cg(N)$ för en konstant $c$ och tillräckligt stora värden av $N$. En
formell
definition av beteckningen kan man hitta i de flesta böcker om design av
algoritmer eller diskret matematik. }.
%

Uttrycken i den yttersta snurran exekveras exakt N-gånger och det i mittersta
snurran exekveras som mest N-gånger i varje exekvering av den yttersta
snurran. Multiplicerar dessa två faktorer inuti den mittersta snurran och visar
att
dessa fyra rader exekveras $O(N^2)$ antal gånger. Snurran i dessa fyra rader
exekveras aldrig mer än $N$-gånger. Detta ger kostnad på algoritmen lika med
 $O(N)$. Om kostnaden multipliceras per antalet gånger som den innersta snurran
körs får vi kostnaden för hela programmet och som även är proportionerligt
till$N^2$. Så detta kan vi kalla för en kvadratisk algoritm. Med kostnad kan vi
tänka oss tiden det tar att köra koden. Nedan kan vi se koden
för Algoritm 2:
%
\begin{center}
%
\begin{algorithmic} 
%
\newpage
%
\STATE{MaxSoFar := 0.0}
\FOR{ L := 1 to N} 
	\FOR{ U := 1 to N}
   		\STATE{Sum := 0.0}
   	\FOR{U := L to N}
  		  \STATE{Sum := Sum + X[I]}
  	 \ENDFOR
  	 \STATE{/* Sum behåller nu summan av $X[L..U]$} */
  	 \STATE{MaxSoFar := max(MaxSoFar, Sum)}
 	 \ENDFOR
 \ENDFOR
\end{algorithmic}
%
\end{center}
%
En kort sammanfattad förklaring angående Algoritm 2 är att den sparar variabler
i sina
element dessa element är dom som hittills beräknats för att få fram summan på 
deltalföljden som samtidigt har beräknats.
%

\subsection{Algoritm 2b}


Dessa enkla steg illustrerar tekniken av “big-oh”, analys av tiden som det
tar att köra och andra fördelar men även nackdelar. Den största nackdelen med
denna är att vi fortfarande inte vet exakt hur lång tid det tar programmet för
en
särskild input. Vi vet bara att antalet steg det tar att exekveras är $O(N^3)$.
Två stycken fördelar med denna metod är att den ofta kompenserar för sina
nackdelar, “big-oh” analyser är ofta användbara när man utföra sådant som
vi beskrev tidigare. Sedan är den ungefärliga tiden ofta tillräcklig för att
utföra den uträckning som används för att bestämma om ett program är
tillräckligt effektivt för den givna uppgiften. Nedan kan vi se koden för 
Algoritm 2b:
%
\begin{center}
%
\begin{algorithmic}
%
\STATE{CumArray[0] := 0.0}
\FOR{I := 1 to N}
 	\STATE{CumArray[I] := CumArray[I - 1] + X[I]}
\STATE{MaxSoFar := 0.0}
\FOR{ L := 1 to N }
  	\FOR{ U := L to N }
		\STATE{ Sum := CumArray[U] - CumArray[L - 1]}
  	 \ENDFOR
  	 \STATE{/* Sum innehåller nu $X[L..U]$} */
  	 \STATE{MaxSoFar := max(MaxSoFar, Sum)}
 	 \ENDFOR
 \ENDFOR
\end{algorithmic}
%
\end{center}
%
En kort sammanfattning kring Algoritm 2b är att den beräknar summan av alla  
föregående element och spara dem på deras dedikerade plats i CumArray'en.
Detta med ett så kallat index. För att räkna ut deltalföljden plockar 
den fram elementets index och söker igenom i CumArray'en och ersätter summan 
som ligger på det index som tidigare legat på dennes plats.
%


\begin{thebibliography}{99}
%
\bibitem{latexcompanion} 
Jon Bentley. 
\textit{Algorithmic Design Techiniques}. 
Programing Pearls, September 1984.
%
\end{thebibliography}
%
\end{document}
